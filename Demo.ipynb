{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "import numpy.linalg as LA\n",
    "import copy\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "############ path of Font #############\n",
    "\n",
    "fontpath = '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc'\n",
    "fontprop = fm.FontProperties(fname=fontpath, size=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
    "model = BertModel.from_pretrained('klue/bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(x, y):\n",
    "    return np.dot(x, y) / math.sqrt(np.dot(x, x) * np.dot(y, y))\n",
    "\n",
    "def s_wAB(w, A, B):\n",
    "    meana = 0\n",
    "    meanb = 0\n",
    "    for a in A:\n",
    "        meana +=cossim(w,a)\n",
    "    for b in B:\n",
    "        meanb +=cossim(w,b)\n",
    "    meana /= len(A)\n",
    "    meanb /= len(B)\n",
    "    return meana-meanb\n",
    "    \n",
    "def mean_s_wAB(X, A, B):\n",
    "    mean = 0\n",
    "    for x in X:\n",
    "        mean += s_wAB(x, A, B)\n",
    "    return mean/len(X)\n",
    "\n",
    "def stdev_s_wAB(X, A, B):\n",
    "    T = []\n",
    "    for x in X:\n",
    "        T.append(s_wAB(x,A,B))\n",
    "    return statistics.stdev(T)\n",
    "\n",
    "def effect_size(X,Y,A,B):\n",
    "    x = mean_s_wAB(X,A,B)\n",
    "    y= mean_s_wAB(Y,A,B)\n",
    "    \n",
    "    return (x-y)/stdev_s_wAB(X+Y,A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_seat(targetName, attrName, bias_subspace=[]):\n",
    "    X = []\n",
    "    Y = []\n",
    "    A = []\n",
    "    B = []\n",
    "\n",
    "    with open(targetName+\"/\"+\"target_X.txt\") as f:\n",
    "        for line in f.readlines():\n",
    "            v = model((tokenizer(line.strip(), return_tensors=\"pt\")).input_ids).last_hidden_state[0][0].detach().numpy()  \n",
    "            hv = v*0\n",
    "            for bias_vector in bias_subspace:\n",
    "                hv = hv + np.dot(bias_vector, v)*bias_vector\n",
    "           \n",
    "            v2=v-hv\n",
    "            X.append(v2/LA.norm(v2))\n",
    "            \n",
    "\n",
    "    with open(targetName+\"/\"+\"target_Y.txt\") as f:\n",
    "        for line in f.readlines():\n",
    "            v = model((tokenizer(line.strip(), return_tensors=\"pt\")).input_ids).last_hidden_state[0][0].detach().numpy()  \n",
    "            hv = v*0\n",
    "            for bias_vector in bias_subspace:\n",
    "                hv = hv + np.dot(bias_vector, v)*bias_vector\n",
    "           \n",
    "            v2=v-hv\n",
    "            Y.append(v2/LA.norm(v2))\n",
    "\n",
    "    with open(attrName+\"/\"+\"attr_A.txt\") as f:\n",
    "        for line in f.readlines():\n",
    "            v = model((tokenizer(line.strip(), return_tensors=\"pt\")).input_ids).last_hidden_state[0][0].detach().numpy()  \n",
    "            hv = v*0\n",
    "            for bias_vector in bias_subspace:\n",
    "                hv = hv + np.dot(bias_vector, v)*bias_vector\n",
    "            \n",
    "            v2=v-hv\n",
    "            A.append(v2/LA.norm(v2))\n",
    "\n",
    "    with open(attrName+\"/\"+\"attr_B.txt\") as f:\n",
    "        for line in f.readlines():\n",
    "            v = model((tokenizer(line.strip(), return_tensors=\"pt\")).input_ids).last_hidden_state[0][0].detach().numpy()  \n",
    "            hv = v*0\n",
    "            for bias_vector in bias_subspace:\n",
    "                hv = hv + np.dot(bias_vector, v)*bias_vector\n",
    "            \n",
    "            v2=v-hv\n",
    "            B.append(v2/LA.norm(v2))\n",
    "    \n",
    "    \n",
    "    if(len(X)!=len(Y)):\n",
    "        print(\"length of X and Y should be the same\")\n",
    "        assert()\n",
    "\n",
    "    \n",
    "    if(len(A)!=len(B)):\n",
    "        print(\"length of A and B are recommended to be the same\")\n",
    "        \n",
    "    return effect_size(X,Y,A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04021164 -0.00959634  0.04158581 ... -0.00519618 -0.01959306\n",
      "  -0.0421946 ]\n",
      " [-0.02284452  0.0072754   0.00746128 ... -0.02240542  0.03168522\n",
      "  -0.01751719]\n",
      " [-0.05221439 -0.00474177  0.02527822 ...  0.02411237  0.0505751\n",
      "  -0.04409362]\n",
      " ...\n",
      " [-0.00406621 -0.05929296 -0.00297973 ...  0.04360102  0.08920283\n",
      "   0.03231892]\n",
      " [-0.08230173 -0.01784659 -0.03371129 ...  0.01286659 -0.03928048\n",
      "  -0.03498467]\n",
      " [-0.02999273  0.07737284 -0.02323433 ... -0.01267975  0.02910168\n",
      "  -0.04029364]]\n"
     ]
    }
   ],
   "source": [
    "bias_subspace = torch.load(\"bias_subspace_d2_opt_20.pt\")\n",
    "print(bias_subspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target  name_phx  + attr  specialty_px  original SEAT score :  0.3751922434255145\n",
      "target  name_phx  + attr  specialty_px  debiased SEAT score :  -0.036792635490124674\n",
      "\n",
      "target  name_phx  + attr  job_px  original SEAT score :  0.955967604637584\n",
      "target  name_phx  + attr  job_px  debiased SEAT score :  0.8998437847817552\n",
      "\n",
      "target  name_phx  + attr  like_px  original SEAT score :  0.8354605087978312\n",
      "target  name_phx  + attr  like_px  debiased SEAT score :  0.7027839570029614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targetNames = [\"name\"]\n",
    "attrNames = [\"job\",\"hobby\",\"specialty\"]\n",
    "for targetName in targetNames:\n",
    "    for attrName in attrNames:\n",
    "        print(\"target \",targetName,\" + attr \",attrName, \" original SEAT score : \",test_seat(targetName,attrName))\n",
    "        print(\"target \",targetName,\" + attr \",attrName, \" debiased SEAT score : \",test_seat(targetName,attrName,bias_subspace))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "mainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
